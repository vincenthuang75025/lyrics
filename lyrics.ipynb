{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "38-PqwyJyd8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcDgF1xccfqb",
        "outputId": "444c1889-2e13-4399-bc7d-7f6feb3166f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lyrics'...\n",
            "remote: Enumerating objects: 22867, done.\u001b[K\n",
            "remote: Counting objects: 100% (2344/2344), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1073/1073), done.\u001b[K\n",
            "remote: Total 22867 (delta 1035), reused 2258 (delta 984), pack-reused 20523\u001b[K\n",
            "Receiving objects: 100% (22867/22867), 4.24 MiB | 13.82 MiB/s, done.\n",
            "Resolving deltas: 100% (11814/11814), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Lyrics/lyrics.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "metadata": {
        "id": "r5_YF01qcinq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_char(char):\n",
        "    if ord(char) < 128:\n",
        "        return char\n",
        "    if char == \"’\":\n",
        "        return \"'\"\n",
        "    return \" \"\n",
        "\n",
        "def get_lyrics(f):\n",
        "    fin = open(f, 'r+')\n",
        "    curr_verse = \"\"\n",
        "    verses = []\n",
        "    metadata = False\n",
        "    for line in fin:\n",
        "        if line == \"\\n\":\n",
        "            if curr_verse != \"\":\n",
        "                clean_verse = ''.join([get_char(i) for i in curr_verse])\n",
        "                verses.append(clean_verse)\n",
        "                curr_verse = \"\"\n",
        "        elif len(line) > 5 and line[:5] == \"_____\":\n",
        "            if curr_verse != \"\":\n",
        "                clean_verse = ''.join([get_char(i) for i in curr_verse])\n",
        "                verses.append(clean_verse)\n",
        "            return verses\n",
        "        else:\n",
        "            if line != \"\\n\":\n",
        "                curr_verse += line\n",
        "    if curr_verse != \"\":\n",
        "        verses.append(curr_verse)\n",
        "    return verses\n",
        "\n",
        "def extract_english_lyrics(remove_if_contains = set()):\n",
        "    word_counts = defaultdict(int)\n",
        "    file_count = 0\n",
        "    song_lyrics = {}\n",
        "    filepaths = []\n",
        "    for path, currentDirectory, files in os.walk(\"lyrics/database\"):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(path, file)\n",
        "            filepaths.append(filepath)\n",
        "    for filepath in sorted(filepaths):\n",
        "        lyrics = get_lyrics(filepath)\n",
        "        removed = False\n",
        "        word_list = []\n",
        "        for lyric in lyrics:\n",
        "            for line in lyric.split('\\n'):\n",
        "              clean_line = ''.join([get_char(i) for i in line])\n",
        "              for word in clean_line.split(\" \"):\n",
        "                  word_list.append(word.lower())\n",
        "        for word in remove_if_contains:\n",
        "            if not removed and word in word_list:\n",
        "                removed = True\n",
        "                os.remove(filepath)\n",
        "        if not removed:\n",
        "            file_count += 1\n",
        "            for word in word_list:\n",
        "                word_counts[word] += 1\n",
        "            song_lyrics[filepath] = lyrics\n",
        "\n",
        "\n",
        "    words = []\n",
        "    for word in word_counts:\n",
        "        words.append((word_counts[word], word))\n",
        "    words.sort(reverse=True)\n",
        "    return song_lyrics, {pair[1]: i for i, pair in enumerate(words)}\n",
        "song_lyrics, sorted_words = extract_english_lyrics(\n",
        "    {'ich', 'und', 'der', 'du', 'das', 'wir', 'nicht', 'ist', 'es', 'ein', \n",
        "     'auf', 'zu', 'sie', 'mich', 'doch', 'wenn', 'dich', 'für', 'wie', 'uns', \n",
        "     'nur', 'sind', 'mir', 'noch'}\n",
        ")\n",
        "\n",
        "def lyrics_to_songs(song_lyrics):\n",
        "    lyric_to_song = {}\n",
        "    song_to_lyric = {}\n",
        "    for song in song_lyrics:\n",
        "        lyrics = song_lyrics[song]\n",
        "        for i, lyric in enumerate(lyrics):\n",
        "            if lyric not in lyric_to_song:\n",
        "                lyric_to_song[lyric] = (song, i)\n",
        "                song_to_lyric[(song, i)] = lyric\n",
        "    return lyric_to_song, song_to_lyric\n",
        "\n",
        "lyric_to_song, song_to_lyric = lyrics_to_songs(song_lyrics)"
      ],
      "metadata": {
        "id": "rInKV_gyckAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "wqxL8iAZyy-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEKMUysXICEu",
        "outputId": "480af216-b03d-4502-de8d-ddab37c90a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "model.eval()\n",
        "\n",
        "def bert(sentence, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "    token_vecs = hidden_states[-2][0]\n",
        "\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "    return sentence_embedding.detach().numpy()\n",
        "\n",
        "def bert_embedding(lyric):\n",
        "    return bert(lyric, model, tokenizer)\n",
        "\n",
        "lyric_to_vector, vector_to_lyric = get_verse_vectors(bert_embedding, song_lyrics)\n",
        "\n",
        "len(lyric_to_vector), len(vector_to_lyric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxyT6SE2H4ZT",
        "outputId": "0dd53400-8c82-4258-f95f-ff7d09d437c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17947, 17890)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_vectors(lyric_to_vector):\n",
        "    np.savez_compressed('lyric_vectors_bert.npz',**lyric_to_vector)\n",
        "\n",
        "def load_vectors(filepath):\n",
        "    loader=np.load(filepath)\n",
        "    data = {}\n",
        "    for lyrics in song_lyrics.values():\n",
        "        for lyric in lyrics:\n",
        "            data[lyric] = loader[lyric]\n",
        "    return data\n",
        "\n",
        "## This takes around 1 minute to load and save (and 30MB), whereas running spacy takes around 5 minutes. Up to you to decide what to use.\n",
        "\n",
        "# save_vectors(lyric_to_vector)\n",
        "lyric_to_vector = load_vectors('lyric_vectors_bert.npz')\n",
        "vector_to_lyric = {tuple(lyric_to_vector[lyric].tolist()): lyric for lyric in lyric_to_vector}"
      ],
      "metadata": {
        "id": "SjWuVS-GeET6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes around 15 seconds to run.\n",
        "def get_knn(lyric_to_vector):\n",
        "    lyric_indices = {}\n",
        "    count = 0\n",
        "    for lyric in lyric_to_vector:\n",
        "        lyric_indices[lyric] = count\n",
        "        count += 1\n",
        "    neigh = NearestNeighbors(n_neighbors=30)\n",
        "    data = [vec for vec in lyric_to_vector.values()]\n",
        "    neigh.fit(data)\n",
        "    neighbors = neigh.kneighbors_graph(data)\n",
        "    row, col = neighbors.nonzero()\n",
        "    return row, col, lyric_indices\n",
        "\n",
        "knn_rows, knn_cols, lyric_indices = get_knn(lyric_to_vector)\n",
        "\n",
        "def index_lyrics(lyric_indices):\n",
        "    index_lyric = {}\n",
        "    for lyric in lyric_indices:\n",
        "        index_lyric[lyric_indices[lyric]] = lyric\n",
        "    return index_lyric\n",
        "index_lyric = index_lyrics(lyric_indices)"
      ],
      "metadata": {
        "id": "xGHDkUX2mYc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "kc2MxQVUzBsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(lyric):\n",
        "    lines = lyric.split(\"\\n\")\n",
        "    word_set = set()\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            index = sorted_words[word.lower()]\n",
        "            if index >= 23:\n",
        "                word_set.add(index)\n",
        "    return word_set\n",
        "\n",
        "def get_bow_sets(bow, song_lyrics):\n",
        "    lyric_to_vector = dict()\n",
        "    for song in song_lyrics:\n",
        "        lyrics = song_lyrics[song]\n",
        "        for lyric in lyrics:\n",
        "            vector = bow(lyric)\n",
        "            lyric_to_vector[lyric] = vector\n",
        "    return lyric_to_vector\n",
        "\n",
        "lyric_to_bow = get_bow_sets(bag_of_words, song_lyrics)"
      ],
      "metadata": {
        "id": "XBrepdicr8d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes 13m to run.\n",
        "def bow_similarity(set1, set2):\n",
        "    return len(set1.intersection(set2)) / (len(set1.union(set2))+1)\n",
        "\n",
        "def get_bow_knn(lyric_to_bow):\n",
        "    bow_knn = {}\n",
        "    count = 0\n",
        "    for lyric in lyric_to_bow:\n",
        "        lyric_knns = []\n",
        "        for other_lyric in lyric_to_bow:\n",
        "            lyric_knns.append((bow_similarity(lyric_to_bow[lyric], lyric_to_bow[other_lyric]), \n",
        "                               lyric_to_song[other_lyric]))\n",
        "            if len(lyric_knns) == 60:\n",
        "                lyric_knns.sort()\n",
        "                lyric_knns = lyric_knns[30:]\n",
        "        bow_knn[lyric_to_song[lyric]] = lyric_knns\n",
        "    return bow_knn\n",
        "\n",
        "bow_knn = get_bow_knn(lyric_to_bow)"
      ],
      "metadata": {
        "id": "P5oFlTAI2819"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Similarity Metrics"
      ],
      "metadata": {
        "id": "rKHo-ftbzYwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_lyrics(bow_knn, knn_rows, knn_cols):\n",
        "    keys = list(bow_knn.keys())\n",
        "    similar_lyric = {}\n",
        "    bonus = {}\n",
        "    for key in keys:\n",
        "        i = 0\n",
        "        val = sorted(bow_knn[key], reverse=True)\n",
        "        while i < len(val) and key[0] == val[i][1][0]:\n",
        "            i += 1\n",
        "        if i < len(val) and val[i][0] >= 0.2 and key[0] != val[i][1][0]:\n",
        "            similar_lyric[key] = val[i][1]\n",
        "            bonus[key] = val[i][0]\n",
        "        else:\n",
        "            lyric = song_to_lyric[key]\n",
        "            i = lyric_indices[lyric]\n",
        "            row = knn_rows[i]\n",
        "            j = 0\n",
        "            col = knn_cols[i+j]\n",
        "            row_lyric = index_lyric[row]\n",
        "            col_lyric = index_lyric[col]\n",
        "            while lyric_to_song[row_lyric][0] == lyric_to_song[col_lyric][0]:\n",
        "                j += 1\n",
        "                col = knn_cols[i+j]\n",
        "                row_lyric = index_lyric[row]\n",
        "                col_lyric = index_lyric[col]\n",
        "            similar_lyric[key] = lyric_to_song[col_lyric]\n",
        "            bonus[key] = np.dot(lyric_to_vector[row_lyric], lyric_to_vector[col_lyric])/(\n",
        "            np.linalg.norm(lyric_to_vector[row_lyric]) * np.linalg.norm(lyric_to_vector[row_lyric]))\n",
        "    return bonus, similar_lyric\n",
        "bonus, similar_lyric = get_similar_lyrics(bow_knn, knn_rows, knn_cols)"
      ],
      "metadata": {
        "id": "SZK-qgSoMSW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markdown Generation"
      ],
      "metadata": {
        "id": "r05yULnVzgr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import os\n",
        "\n",
        "def urlize(s, spacer):\n",
        "    return s.lower().translate(str.maketrans('', '', string.punctuation)).replace(\" \", spacer)\n",
        "\n",
        "tags = set()\n",
        "\n",
        "def generate_markdown(filename):\n",
        "    # generate md file for quartz\n",
        "    lyrics = song_lyrics[filename]\n",
        "    fin = open(filename, 'r+')\n",
        "    metadata = False\n",
        "    metadata_map = {}\n",
        "    for line in fin.readlines():\n",
        "        if not metadata and line[:5] == '_____':\n",
        "            metadata = True\n",
        "        elif metadata:\n",
        "            key = \"\"\n",
        "            val = \"\"\n",
        "            begin_split = False\n",
        "            end_split = False\n",
        "            for char in line.strip():\n",
        "                if not begin_split and char == \" \":\n",
        "                    begin_split = True\n",
        "                elif not begin_split:\n",
        "                    key += char\n",
        "                elif begin_split and not end_split and char != \" \":\n",
        "                    end_split = True\n",
        "                    val += char\n",
        "                elif end_split:\n",
        "                    val += char\n",
        "            metadata_map[key] = val.replace(\":\", \"\")\n",
        "    md_str = \"---\\n\"\n",
        "    md_str += f'title: \"{metadata_map[\"Name\"]}\"\\n'\n",
        "    md_str += \"tags:\\n\"\n",
        "    if 'Artist' in metadata_map:\n",
        "        md_str += f'- {metadata_map[\"Artist\"]}\\n'\n",
        "    if 'Album' in metadata_map:\n",
        "        md_str += f'- {metadata_map[\"Album\"]}\\n'\n",
        "    md_str += \"---\\n\"\n",
        "    for i, lyric in enumerate(lyrics):\n",
        "        md_str += \"&nbsp;\\n\"\n",
        "        if (filename, i) not in similar_lyric:\n",
        "            for line in lyric.strip().split(\"\\n\"):\n",
        "                md_str += f'#### {line}\\n'\n",
        "        else:\n",
        "            similar = similar_lyric[(filename, i)]\n",
        "            other_lyric = song_to_lyric[similar]\n",
        "            similar_ref = similar_lyric[similar]\n",
        "            tag = urlize(other_lyric.split(\"\\n\")[0], \"-\")\n",
        "            while len(tag) > 0 and tag[-1] == '-':\n",
        "                tag = tag[:-1]\n",
        "            tag = tag + \"-vyl-wnanory\"\n",
        "            elements = []\n",
        "            for element in similar[0][16:].split(\"/\"):\n",
        "                elements.append(urlize(element, \"_\"))\n",
        "            link = \"songs/\" + \"/\".join(elements) + \".md\"\n",
        "            for j, line in enumerate(lyric.strip().split(\"\\n\")):\n",
        "                while len(line) > 0 and line[-1] == ' ':\n",
        "                    line = line[:-1]\n",
        "                md_str += f'#### [[{link}#{tag}|{line}]]'\n",
        "                anchor = urlize(line, \"-\") + \"-vyl-wnanory\"\n",
        "                if j == 0 and anchor not in tags:\n",
        "                    tags.add(anchor)\n",
        "                    md_str += \" {#\" + anchor + \"}\"\n",
        "                md_str += \"\\n\"\n",
        "    filename_elements = []\n",
        "    for element in filename[16:].split(\"/\"):\n",
        "        filename_elements.append(urlize(element, \"_\"))\n",
        "    new_filename = \"songs/\" + \"/\".join(filename_elements) + \".md\"\n",
        "    os.makedirs(os.path.dirname(new_filename), exist_ok=True)\n",
        "    with open(new_filename, \"w\") as f:\n",
        "        f.write(md_str)\n",
        "\n",
        "def generate_markdowns():\n",
        "    # traverse all files and generate md for each\n",
        "    filepaths = []\n",
        "    for path, currentDirectory, files in os.walk(\"lyrics/database\"):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(path, file)\n",
        "            filepaths.append(filepath)\n",
        "    for filepath in sorted(filepaths):\n",
        "        generate_markdown(filepath)\n",
        "\n",
        "generate_markdowns()"
      ],
      "metadata": {
        "id": "L8HYZC0yRTJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"songs\", 'zip', \"songs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZsCXU7dHebXs",
        "outputId": "79c67d54-e3d6-4fbc-9db5-8a172dd36c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/songs.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}